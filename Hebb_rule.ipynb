# ðŸ§  Hebbian Rule + Decision Boundary Visualization (AND Gate)

In this notebook, we train a simple Hebbian learning model on the **bipolar AND gate** and visualize the decision boundary **before and after training**.

---

## ðŸ“¥ Import Libraries

```python
import numpy as np
import matplotlib.pyplot as plt

def activationFunction(x):
    return 1 if x >= 0 else -1

def HebbRule(X, T, W, b):
    print("Initial Weights:", W, "| Initial Bias:", b)
    print("-" * 50)

    for i in range(len(X)):
        y_in = np.dot(W, X[i]) + b
        t_pred = activationFunction(y_in)

        W = W + X[i] * T[i]   # Hebbian update rule
        b = b + T[i]

        print(f"Input: {X[i]}, Target: {T[i]}, Predicted: {t_pred}")
        print(f"Updated Weights â†’ {W}, Updated Bias â†’ {b}")
        print("-" * 50)

    return W, b

# Inputs: bipolar values
X = np.array([
    [1, 1],
    [1, -1],
    [-1, 1],
    [-1, -1]
])

# Target output for AND gate (bipolar)
T = np.array([1, -1, -1, -1])

# Initial weights and bias
W_initial = np.array([1, 1], dtype=float)
b_initial = 1

def plot_decision_boundary(W, b, title):
    x_vals = np.linspace(-2, 2, 100)
    y_vals = -(W[0] * x_vals + b) / W[1]  # Line: w1*x + w2*y + b = 0

    # Plot
    plt.figure(figsize=(6, 6))
    plt.plot(x_vals, y_vals, label="Decision Boundary", linewidth=2)

    # Plot input points
    for i, point in enumerate(X):
        color = "green" if T[i] == 1 else "red"
        plt.scatter(point[0], point[1], color=color, s=120)
        plt.text(point[0]+0.1, point[1], f"{T[i]}")

    plt.axhline(0, linewidth=1)
    plt.axvline(0, linewidth=1)
    plt.title(title)
    plt.grid(True)
    plt.xlim(-2, 2)
    plt.ylim(-2, 2)
    plt.show()

plot_decision_boundary(W_initial, b_initial, "Before Training (Decision Boundary)")

final_W, final_b = HebbRule(X, T, W_initial.copy(), b_initial)
print("\nFinal weights:", final_W)
print("Final bias:", final_b)

plot_decision_boundary(final_W, final_b, "After Training (Decision Boundary)")

def predict(x):
    y = np.dot(final_W, x) + final_b
    return activationFunction(y)

print("\nTesting Model:")
for i in range(len(X)):
    print(f"Input: {X[i]}, Output: {predict(X[i])}, Expected: {T[i]}")
